{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voting classifiers\n",
    "\n",
    "Suppose you have trained 5 classifiers to achive 80% accuracy. Agrregate predictions of each using simple voting. Even if the underlying classifiers are weak, the voting still provides a strong learner (high accuracy). This is akin to [Wisdom of the Crowd](https://en.wikipedia.org/wiki/Wisdom_of_the_crowd). The key for it work is each predictor should be independent and trained on different alorithms. Or use different training dataset. They will make different errors and increase accuracy.\n",
    "\n",
    "- Hard Voting: Use maximum of the votes\n",
    "- Soft voting: use the probabilities of the underlying models and averge them. The ude the highest probability to get prediction. \n",
    "\n",
    "## Different Algorithms\n",
    "\n",
    "This example explores different alogrithms trained on same data.\n",
    "\n",
    "### VotingClassifier\n",
    "\n",
    " - Type: Averaging Ensemble\n",
    " - Hard Voting: $\\hat{y} = \\arg\\max_{c \\in C} \\sum_{i=1}^{n} \\mathbb{I}(\\hat{y}i = c)$ \n",
    " - Soft Voting: $\\hat{y} = \\arg\\max{c \\in C} \\sum_{i=1}^{n} w_i \\cdot P_i(c)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_moons(n_samples=500, noise=0.3, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of LogisticRegression:  0.864\n",
      "Accuracy of RandomForestClassifier:  0.896\n",
      "Accuracy of SVC:  0.896\n",
      "Accuracy of VotingClassifier:  0.912\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "log_clf = LogisticRegression(solver=\"lbfgs\", random_state=42)\n",
    "rnd_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "svm_clf = SVC(gamma=\"scale\", probability=True, random_state=42)\n",
    "\n",
    "# Ceate a hard voting classifier\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "    voting='hard')\n",
    "\n",
    "# Predict and score\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(\"Accuracy of \" + clf.__class__.__name__ +\": \", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different Datasets\n",
    "\n",
    "## Bagging Classifiers\n",
    "This example explores same alogrithms trained on different data. Create samples either using replacement (=bootstrap) called __bagging__ or no-replacement called __pasting__. The replacement means that the sample kept back in the bag so can be chosen again. So bagging may produce duplicate training samples. \n",
    "\n",
    "So for a training sample size $m$ the probability that is NOT selected in a draw is $ 1 - \\frac{1}{m}$ and in $m$ draws is $ (1 - \\frac{1}{m})^m$. So $\\lim\\limits_{m \\to \\infty}(1 - \\frac{1}{m})^m = \\frac{1}{e} \\approx 37\\%$. These 37% are caled out-of-bag samples, note that these are not same for all predictors. We can set `oob_score=True` for `BaggingClassifier()` to do a evaluation. It averages the oob score for each predictor to arrive at a final oob-score.\n",
    "\n",
    "To sample features you can use `max_features` and `bootstrap_features`.\n",
    "\n",
    "- Type: Bagging (Bootstrap Aggregation) \n",
    "- Objective: $\\hat{f}(x) = \\frac{1}{T} \\sum_{t=1}^{T} h_t(x)$ (No loss function minimized; variance reduction through averaging)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of bagging classifier:  0.92\n",
      "OOB score: 0.925\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Default voting is soft, train 500 decision trees and use 100 samples in training each\n",
    "# tree with replacement bootstrap=True, random_state=42\n",
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(max_leaf_nodes=16, max_features=\"sqrt\"),\n",
    "    n_estimators=500,\n",
    "    max_samples=100,\n",
    "    random_state=42,\n",
    "    bootstrap=True,\n",
    "    oob_score=True\n",
    ").fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "print(\"Accuracy of bagging classifier: \", accuracy_score(y_test, y_pred))\n",
    "print(f\"OOB score: {bag_clf.oob_score_:0.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of decision tree:  0.856\n"
     ]
    }
   ],
   "source": [
    "# Individual decision trees\n",
    "tree_clf = DecisionTreeClassifier(random_state=42)\n",
    "tree_clf.fit(X_train, y_train)\n",
    "y_pred_tree = tree_clf.predict(X_test)\n",
    "print(\"Accuracy of decision tree: \", accuracy_score(y_test, y_pred_tree))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForestClassifier\n",
    "\n",
    "Is an out-of-the-box optimized Bagging Classifier for ensemble of `DecisionTreeClassifier`. It has all hyper-parameters of DecisionTreeClassifier (tree-growing controls) and BaggingCLassifier (controling ensemble). It adds extra randomness in growing trees. It searches for best feature among a random subset of features per split. It also does a \n",
    "stronger de-correlation via row-and-coliumn sampling. Where as BaggingClassifier does not do both of these.\n",
    "\n",
    "It results in hugher tree diversity which trades higher bias for lower variance. Generaly yields better overall model. \n",
    "\n",
    "__Note:__ `feature_importances_` contains name and normalized importance of the features. \n",
    "\n",
    "- Type: Bagging + Random Feature Selection\n",
    "- Split Criterion: Gini: $Gini(p) = 1 - \\sum_{k=1}^{K} p_k^2$ \n",
    "- Entropy: $Entropy = - \\sum p_k \\log p_k$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of random forest:  0.92\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500,\n",
    "                                 max_leaf_nodes=16,\n",
    "                                 max_samples=100,\n",
    "                                 random_state=42)\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rnd_clf.predict(X_test)\n",
    "print(\"Accuracy of random forest: \", accuracy_score(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(1.0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare the predictions of the BaggingClassifier and RandomForestClassifier, 100% match\n",
    "np.sum(y_pred == y_pred_rf) / len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 0, 0, 1, 1]), array([0, 0, 0, 1, 1]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First 5 predictions\n",
    "y_pred[:5], y_pred_rf[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.42755541, 0.57244459]), np.float64(1.0))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalized feature importance\n",
    "rnd_clf.feature_importances_, np.sum(rnd_clf.feature_importances_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
