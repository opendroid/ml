{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aae4493c",
   "metadata": {},
   "source": [
    "# Image Search\n",
    "\n",
    "Problem: Given a image find the top n images that a user will most likely click."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f5ee0908",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, Concatenate, Flatten, Dot\n",
    "# NEW: Import a pre-trained vision model\n",
    "from tensorflow.keras.applications import MobileNetV2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10dca73",
   "metadata": {},
   "source": [
    "### Create Test data\n",
    "\n",
    "1. List of 512 users\n",
    "2. Images in databse say 2_000\n",
    "3. Training samples 50_000\n",
    "\n",
    "\n",
    "#### Create unqiue images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7a4012d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_USERS = 1_000  # Total unique users in the system.\n",
    "NUM_IMAGES = 5_000  # Total unique images in your database.\n",
    "# e.g., 100 different object labels like 'dog', 'car', 'beach'.\n",
    "NUM_LABELS = 100\n",
    "# The number of training examples (past user interactions).\n",
    "NUM_SAMPLES = 200_000\n",
    "\n",
    "# Define the dimensionality of our pre-computed embeddings.\n",
    "# This would be determined by the vision model you used (e.g., MobileNetV2 outputs 1280).\n",
    "EMBEDDING_DIM = 1_280"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7e4ee94e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulated 5000 unique images with 1280-dim embeddings.\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Simulate the Pre-computed \"Pools\" of Data ---\n",
    "# In a real project, you would load this data from your database.\n",
    "\n",
    "# A pool of pre-computed embeddings for every unique image.\n",
    "unique_image_embeddings = np.random.rand(\n",
    "    NUM_IMAGES, EMBEDDING_DIM).astype(np.float32)\n",
    "\n",
    "# A pool of pre-computed metadata (e.g., a primary object label) for each image.\n",
    "unique_image_labels = np.random.randint(0, NUM_LABELS, size=NUM_IMAGES)\n",
    "\n",
    "print(\n",
    "    f\"Simulated {NUM_IMAGES} unique images with {EMBEDDING_DIM}-dim embeddings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3e2a8333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Simulate the Training Data (User Interactions) ---\n",
    "# This creates the 200,000 training samples.\n",
    "\n",
    "# For each sample, randomly assign a user, a query image, and a candidate image.\n",
    "user_ids = np.random.randint(0, NUM_USERS, size=NUM_SAMPLES)\n",
    "query_image_ids = np.random.randint(0, NUM_IMAGES, size=NUM_SAMPLES)\n",
    "candidate_image_ids = np.random.randint(0, NUM_IMAGES, size=NUM_SAMPLES)\n",
    "# Position in search results (1-20)\n",
    "positions = np.random.randint(1, 21, size=NUM_SAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b1edecd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Gather the Actual Data for Training Using the IDs ---\n",
    "# We use the IDs to pull the corresponding pre-computed data from our unique pools.\n",
    "\n",
    "query_embeddings_for_training = unique_image_embeddings[query_image_ids]\n",
    "candidate_embeddings_for_training = unique_image_embeddings[candidate_image_ids]\n",
    "candidate_labels_for_training = unique_image_labels[candidate_image_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4c5315d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Engineer the 'click' Label ---\n",
    "# We create a logical pattern for the model to learn. A click is more likely if:\n",
    "# 1. The query and candidate images are visually similar.\n",
    "# 2. The candidate image appeared in a high position (e.g., top 5).\n",
    "similarity = np.sum(query_embeddings_for_training *\n",
    "                    candidate_embeddings_for_training, axis=1)\n",
    "click_probability = (similarity > 0.1 * EMBEDDING_DIM) & (positions < 10)\n",
    "clicks = np.array(click_probability, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "35dc50f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1] [109588  90412]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "v, c = np.unique(clicks, return_counts=True)\n",
    "print(v, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1e245be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6. Define the Model Architecture ---\n",
    "\n",
    "# --- Input Layers ---\n",
    "# The model expects pre-computed vectors, not raw images.\n",
    "user_id_input = Input(shape=(1,), name='user_id_input')\n",
    "query_embedding_input = Input(shape=(EMBEDDING_DIM,), name='query_embedding_input')\n",
    "candidate_embedding_input = Input(shape=(EMBEDDING_DIM,), name='candidate_embedding_input')\n",
    "candidate_label_input = Input(shape=(1,), name='candidate_label_input')\n",
    "position_input = Input(shape=(1,), name='position_input')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
