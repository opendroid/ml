{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C93-SkWJoY5b"
   },
   "source": [
    "# __Introduction to BERT and Transformers Library__\n",
    "- BERT stands for Bidirectional Encoder Representations from Transformers.\n",
    "- BERT is pre-trained on a large corpus of unlabeled text, including the entire Wikipedia (that's 2,500 million words!) and the Book Corpus (800 million words).\n",
    "- BERT is based on the Transformer architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6YQ_il2txL-2"
   },
   "source": [
    "## Steps to be followed:\n",
    "1. Import the required libraries\n",
    "2. Analyze the sentiment using the transformer pipeline\n",
    "3. Create text generation\n",
    "4. Create named entity recognition (NER)\n",
    "5. Generate a masked language model using a model and a tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "scQJ3cH8m61w"
   },
   "source": [
    "### Step 1: Import Required Libraries\n",
    "- The code from the transformers import pipeline allows for easy access to pre-trained models and simplified execution of NLP tasks using the transformers library.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "k897DME3x3Wn"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xyz-ai/Developer/python3-code/ml/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Disable oneDNN optimizations to avoid potential minor numerical differences caused by floating-point round-off errors.\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torchvision\n",
    "torchvision.disable_beta_transforms_warning()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bd9WmpIFx4pS"
   },
   "source": [
    "### Step 2: Analyze Sentiment Using Transformer Pipeline\n",
    "\n",
    "- Import the pipeline function from the Transformers library, which enables easy access to pre-trained NLP models\n",
    "- The snippet creates a sentiment analysis pipeline using the pre-trained model and uses it to classify the sentiment of the input text **I hate you**\n",
    "- The result, including the sentiment label and score, is then printed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ruMCIipSm61x"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: NEGATIVE, with score: 0.9991\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "result = classifier(\"I hate you\")[0]\n",
    "print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-frtliQRqC9r"
   },
   "source": [
    "- Perform sentiment analysis on the text **I love you**.\n",
    "- Print the sentiment analysis result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "phseGtlum61y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: POSITIVE, with score: 0.9999\n"
     ]
    }
   ],
   "source": [
    "result = classifier(\"I love you\")[0]\n",
    "print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "MFC7g6L0_vF5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: POSITIVE, with score: 0.9989\n"
     ]
    }
   ],
   "source": [
    "result = classifier(\"The food was not bad.\")[0]\n",
    "print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NS6lz9OAq-ES"
   },
   "source": [
    "**Observation**\n",
    "- The sentiment analysis model is highly confident that the sentiment of the text **I love you** is positive, with a score of 0.9999."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vNEoZm3wrGc_"
   },
   "source": [
    "### Step 3: Create Text Generation\n",
    "- It creates a text generation pipeline using the pipeline function from the Transformers library.\n",
    "- It generates text starting with the provided prompt **As far as I am concerned, I will** using the text generation pipeline, with a maximum length of 50 tokens and without sampling, which is deterministic output.\n",
    "- The generated text is then printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "hjNtv9zVm61y"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to openai-community/gpt2 and revision 607a30d (https://huggingface.co/openai-community/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use mps:0\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'As far as I am concerned, I will be the first to admit that I am not a fan of the idea of'}]\n"
     ]
    }
   ],
   "source": [
    "text_generator = pipeline(\"text-generation\")\n",
    "print(text_generator(\"As far as I am concerned, I will\",\n",
    "      max_new_tokens=15, do_sample=False, truncation=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RkrObi9WrcIL"
   },
   "source": [
    "### Step 4: Create Named Entity Recognition (NER)\n",
    "- It creates a NER pipeline using the pipeline function from the Transformers library.\n",
    "\n",
    "- It applies the NER pipeline to the provided sequence, which is a text containing named entities. The pipeline identifies and extracts named entities such as organization names **Hugging Face Inc.**, locations **New York City**, and others. The extracted entities are then printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "1Zi0w6ZBm61y"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "ner_pipe = pipeline(\"ner\")\n",
    "sequence = \"\"\"Please get me a pepperoni pizza, medium size with pine-apple and cheese \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_y7-t5pnr0lj"
   },
   "source": [
    "- Print the Entities after Performing Named Entity Recognition on the Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "eph1vgCSm61y"
   },
   "outputs": [],
   "source": [
    "for entity in ner_pipe(sequence):\n",
    "    print(entity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Imd2QcVEm61z"
   },
   "source": [
    "\n",
    "### Step 5: Generate Masked Language Model Using a Model and a Tokenizer\n",
    "\n",
    "- Masked Language Modeling Using a Model and a Tokenizer\n",
    "  - Masked language modeling is a task where a model fills in masked tokens in a sequence, improving its understanding of language. It involves predicting missing tokens by considering the context of surrounding words.\n",
    "\n",
    "- The process includes the following steps:\n",
    "  - Instantiate a tokenizer and a model from the checkpoint name.\n",
    "  - Define a sequence with a masked token, placing the tokenizer.mask_token instead of a word.\n",
    "  - Encode that sequence into a list of IDs and find the position of the masked token in that list.\n",
    "  - Retrieve the predictions at the index of the masked token\n",
    "  - Retrieve the top 5 tokens using the PyTorch topk or TensorFlow top_k methods\n",
    "  - Replace the masked token with the tokens and print the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UrNd1zdTvI8K"
   },
   "source": [
    " ### Masked Language Modeling\n",
    "- Import the necessary modules from the transformers library and torch\n",
    "- Load the pre-trained tokenizer and model\n",
    "- Define the input sequence with a masked token\n",
    "- Tokenize the input sequence and convert to tensors\n",
    "- Find the index of the masked token and generate token predictions using the model\n",
    "- Get the indices of the top 5 predicted tokens and print them in the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "fI7dzV5Tm61z"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"distilbert-base-cased\")\n",
    "\n",
    "sequence = (\n",
    "    \"Distilled models are smaller than the models they mimic. Using them instead of the large \"\n",
    "    f\"versions would help {tokenizer.mask_token} our carbon footprint.\"\n",
    ")\n",
    "\n",
    "inputs = tokenizer(sequence, return_tensors=\"pt\")\n",
    "mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "tRdBD9Jl_vF8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help [MASK] our carbon footprint.\n"
     ]
    }
   ],
   "source": [
    "print(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "MfVv8HwA_vF8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101, 12120,  2050,  8683,  1181,  3584,  1132,  2964,  1190,  1103,\n",
      "          3584,  1152, 27180,   119,  7993,  1172,  1939,  1104,  1103,  1415,\n",
      "          3827,  1156,  1494,   103,  1412,  6302,  2555, 10988,   119,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "f8VPoZyL_vF8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101, 12120,  2050,  8683,  1181,  3584,  1132,  2964,  1190,  1103,\n",
      "          3584,  1152, 27180,   119,  7993,  1172,  1939,  1104,  1103,  1415,\n",
      "          3827,  1156,  1494,   103,  1412,  6302,  2555, 10988,   119,   102]])\n"
     ]
    }
   ],
   "source": [
    "print(inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "nKDJObch_vF9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.mask_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "xDHVp8A0_vF9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([0]), tensor([23]))\n"
     ]
    }
   ],
   "source": [
    "print(torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "4ClhIQKb_vF9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([23])\n"
     ]
    }
   ],
   "source": [
    "print(mask_token_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VpoeIy1m_vF9"
   },
   "source": [
    "<br>\n",
    "\n",
    "__Predict the best 5 word that could be at the mask position__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "Dhv_9REF_vF9"
   },
   "outputs": [],
   "source": [
    "token_logits = model(**inputs).logits\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "8be97kFe_vF9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ -6.6732,  -6.6450,  -6.7923,  ...,  -5.5930,  -5.2783,  -5.6559],\n",
      "         [ -6.3221,  -5.6379,  -5.8990,  ...,  -4.6864,  -4.1499,  -5.3507],\n",
      "         [ -5.9863,  -6.0991,  -5.8089,  ...,  -5.2297,  -4.3015,  -6.5971],\n",
      "         ...,\n",
      "         [ -7.8892,  -7.6719,  -7.6357,  ...,  -6.9083,  -5.5853,  -6.2459],\n",
      "         [-14.7710, -14.2714, -14.1642,  ..., -11.4769, -12.1692, -13.1041],\n",
      "         [-14.3695, -13.9839, -13.6330,  ..., -11.2066, -11.6754, -12.7083]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(token_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "Hs5qmDzl_vF-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-5.5502, -5.6790, -5.3256,  ..., -5.4807, -4.5107, -4.2441]],\n",
      "       grad_fn=<IndexBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(mask_token_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "GyewbFu8_vF-"
   },
   "outputs": [],
   "source": [
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "XqM9z7m-_vF-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4851, 2773, 9711, 18134, 4607]\n"
     ]
    }
   ],
   "source": [
    "print(top_5_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "cDkPAknk_vF-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reduce\n",
      "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help reduce our carbon footprint.\n",
      " \n",
      "increase\n",
      "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help increase our carbon footprint.\n",
      " \n",
      "decrease\n",
      "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help decrease our carbon footprint.\n",
      " \n",
      "offset\n",
      "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help offset our carbon footprint.\n",
      " \n",
      "improve\n",
      "Distilled models are smaller than the models they mimic. Using them instead of the large versions would help improve our carbon footprint.\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for token in top_5_tokens:\n",
    "    print(tokenizer.decode([token]))\n",
    "    print(sequence.replace(tokenizer.mask_token, tokenizer.decode([token])))\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cB7sDnvh_qhp"
   },
   "source": [
    "**Observation**\n",
    "- The output provides alternative sentence suggestions by replacing the masked token with different predicted tokens, demonstrating how using distilled models instead of larger ones can impact the carbon footprint."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
