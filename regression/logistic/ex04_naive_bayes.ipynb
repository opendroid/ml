{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Naive Bays Classification\n",
    "\n",
    "Overview of Naive Bayes\n",
    "Naive Bayes is a probabilistic classifier based on Bayes' theorem, assuming features are __conditionally independent__ given the class label. For a given sample with features $x_1, x_2, \\dots x_n $ , the classifier predicts the class $y$ that maximizes the posterior probability:\n",
    "\n",
    "$\n",
    "P(y \\mid x_1, x_2) \\propto P(y) \\cdot P(x_1\\mid y) \\cdot P(x_2\\mid y)\n",
    "$\n",
    "\n",
    "Where:\n",
    "  - $P(y)$ Prior probability of class $y$\n",
    "  - $P(x_i \\mid y)$: liklihood of $x_i$ give $y$\n",
    "\n",
    "\n",
    "For continuous features, Gaussian Naive Bayes assumes each feature follows a normal (Gaussian) distribution This allows us to predict $P(x_i \\mid y)$ using the Gaussian probability density function. \n",
    "\n",
    "### Training\n",
    "During training we calculate for each $y = (0,1)$:\n",
    " - the mean ($\\mu_{x_1,y},\\mu_{x_2,y},\\dots$)\n",
    " - variance ($\\sigma_{x_1,y},\\sigma_{x_2,y},\\dots$) for each feature for each output class.\n",
    " - Class priors $P(y=0)$ and $P(y=1)$\n",
    "\n",
    "The above calculations are stored as class parameters.\n",
    "## Prediction\n",
    "\n",
    "During prediction the parameters are used to compute the posterior for each class:\n",
    "\n",
    "$\n",
    "P(x_i\\mid y=c) = \\frac{1}{\\sqrt{2\\pi\\sigma_{i,y}^2}} \\exp\\left(-\\frac{(x_i - \\mu_{i,y})^2}{2\\sigma_{i,y}^2}\\right)\n",
    "$\n",
    "\n",
    "Where, these are calculated during training:\n",
    " - $\\mu_{i,y}$: Mean of feature $x_i$ for class $y$.\n",
    " - $\\sigma_{i,y}^2$: Variance of feature $x_i$ for class $y$.\n",
    "\n",
    " Then we use the formulae above $P(y \\mid x_1, x_2)$ to calculate overall probablity of class outcome. After that the result is normalized amomg class outcomes as we did not use demoninator above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Dataset\n",
    "\n",
    "Keep only first two features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_breast_cancer()\n",
    "X = data.data[:, :2] # Only two features for experiment\n",
    "y = data.target\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.3, random_state=242, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "The training includes calculating mean and variance for each feature class. We compare these by manually calculating these and using `GaussianNB` logistic class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prior probabilities: P(y=0) = 0.3719, P(y=1) = 0.6281\n",
      "Mean values: mu_0 = [0.88464823 0.44463023], mu_1 = [-0.58503963 -0.32430205]\n",
      "Standard deviation: sigma_0 = [0.91768878 0.73103412], sigma_1 = [0.52567253 0.92684928]\n"
     ]
    }
   ],
   "source": [
    "# Calculate mean and variance for each feature for each class\n",
    "mu_0 = X_train[y_train == 0].mean(axis=0)\n",
    "mu_1 = X_train[y_train == 1].mean(axis=0)\n",
    "\n",
    "sigma_0 = X_train[y_train == 0].std(axis=0)\n",
    "sigma_1 = X_train[y_train == 1].std(axis=0)\n",
    "\n",
    "# prior probabilities\n",
    "P_1 = y_train.mean()\n",
    "P_0 = 1 - P_1\n",
    "\n",
    "# Calculate posterior probabilities\n",
    "print(f\"Prior probabilities: P(y=0) = {P_0:.4f}, P(y=1) = {P_1:.4f}\")\n",
    "print(f\"Mean values: mu_0 = {mu_0}, mu_1 = {mu_1}\")\n",
    "print(f\"Standard deviation: sigma_0 = {sigma_0}, sigma_1 = {sigma_1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: [0 1]\n",
      "Class priors: P(y=0) = 0.3719, P(y=1) = 0.6281\n",
      "Class means: mu_0 = [0.88464823 0.44463023], mu_1 = [-0.58503963 -0.32430205]\n",
      "Class standard deviations: sigma_0 = [0.91768878 0.73103412], sigma_1 = [0.52567253 0.92684928]\n"
     ]
    }
   ],
   "source": [
    "# Create Gaussian Naive Bayes model\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Print parameters\n",
    "print(f\"Classes: {model.classes_}\")\n",
    "print(f\"Class priors: P(y=0) = {model.class_prior_[0]:.4f}, P(y=1) = {model.class_prior_[1]:.4f}\")\n",
    "print(f\"Class means: mu_0 = {model.theta_[0]}, mu_1 = {model.theta_[1]}\")\n",
    "std_dev = np.sqrt(model.var_)\n",
    "print(f\"Class standard deviations: sigma_0 = {std_dev[0]}, sigma_1 = {std_dev[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Predictions Probablilities\n",
    "\n",
    "This section manually calculates the probabilities for each test obervations. And compare them to those calculated by `GaussianNB()` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.65574824e-01, 8.34425176e-01],\n",
       "       [9.99896641e-01, 1.03359317e-04],\n",
       "       [7.77212714e-01, 2.22787286e-01]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xt_0 = X_test[:, 0] # Feature 0\n",
    "xt_1 = X_test[:, 1] # Feature 1\n",
    "# Calculate probabilities for each class, for each feature\n",
    "# Calculate P(x_i|y=0)\n",
    "prob_0_0 = np.exp(-(xt_0 - mu_0[0])**2 / (2 * sigma_0[0]**2)) / (sigma_0[0] * np.sqrt(2 * np.pi))\n",
    "prob_0_1 = np.exp(-(xt_1 - mu_0[1])**2 / (2 * sigma_0[1]**2)) / (sigma_0[1] * np.sqrt(2 * np.pi))\n",
    "\n",
    "# Calculate P(x_i|y=1)\n",
    "prob_1_0 = np.exp(-(xt_0 - mu_1[0])**2 / (2 * sigma_1[0]**2)) / (sigma_1[0] * np.sqrt(2 * np.pi))\n",
    "prob_1_1 = np.exp(-(xt_1 - mu_1[1])**2 / (2 * sigma_1[1]**2)) / (sigma_1[1] * np.sqrt(2 * np.pi))\n",
    "\n",
    "# P = P(y=0) * P(x_i|y=0) * P(x_j|y=0)\n",
    "prob_0_combined = prob_0_0 * prob_0_1 * P_0\n",
    "prob_1_combined = prob_1_0 * prob_1_1 * P_1\n",
    "prob_combined = np.column_stack((prob_0_combined, prob_1_combined))\n",
    "\n",
    "# Normalize to get probabilities\n",
    "prob_combined_normalized = prob_combined / prob_combined.sum(axis=1, keepdims=True)\n",
    "prob_combined_normalized[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.65574824e-01, 8.34425176e-01],\n",
       "       [9.99896641e-01, 1.03359321e-04],\n",
       "       [7.77212714e-01, 2.22787286e-01]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions\n",
    "y_pred = model.predict_proba(X_test)\n",
    "\n",
    "y_pred[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(y_pred, prob_combined_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Remark:__ You can check to see that the prediction probabilities for each class match"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
