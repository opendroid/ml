{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-class Logistic Regression\n",
    "\n",
    "The sklearn LogisticRegression can solve binary and multi-class problems. The multi-class are checked uses `len(unique(y)) > 2` to choose the underlying algorithm. It can be used for:\n",
    "1. Binary: Only one class 0 or 1 \n",
    "2. Multi-class: Each sample can be > 2 classes. For example `y = [0, 1, 2, 3, 0]`\n",
    "    - Use ```multi_class=\"over\" or \"multinomial\"```\n",
    "3. Multilabel: Each sample can be in multiple classes. But each row is a binary vector. Example is a movie is __Action__, __Not romcom__, and __Sci-Fi__. Use `OneVsRestClassifier(LogisticRegression())` for this\n",
    "    ```python\n",
    "    y = [\n",
    "        [1, 0, 1], # Sample 1 belongs to class 0 and 2\n",
    "        [0, 1, 1], # Sample 2 belongs to class 2 and 3\n",
    "        [0, 1, 0], # Sample 3 belongs to class 1\n",
    "    ]\n",
    "    ```\n",
    "4. Multi-output: It is broader term than Multi-label. Each sample can have multiple independent output. The example are predict __minimum__ and __maximum__ temperature or __x__ and __y__ of a object on a image. Use `MultiOutputClassifier(LogisticRegression())` for this\n",
    "    ```python\n",
    "    y = [\n",
    "        [0, 1], # first task class-0, second class-1\n",
    "        [2, 0], # first task class-2, second class-0\n",
    "        [1, 2], # first task class-1, second class-2\n",
    "    ]\n",
    "    ```\n",
    "## Objective\n",
    "The objective if this notebook is to show OVR  (One over rest) strategy used by SKlearn LogisticRegression for multi-label example. That is same as taking each class separately, and create binary classifier. Then choose the one over the rest (with highest probability). To get the apples-to-apples comparision of probability calculations, we need to provide exact same inputs and radom_state in initialization. We shall choose the iris dataset.\n",
    "\n",
    "\n",
    "## Example: Multi-class\n",
    "The example uses iris data-set to solve multi-class classification. \n",
    "\n",
    "The species are: 0 Setosa, 1:Versicolor, 2: Virginica. Create three label columns one for each class for manual OVR\n",
    " - Column 0: [0, 1]: Class-0 Setosa\n",
    " - Column 1: [0, 1]; Class-1 Versicolor\n",
    " - Column 2: [0, 1]; Class-1 Virginica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2]), 'classes = ', 3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y_original = iris.target\n",
    "np.unique(y_original), \"classes = \", len(np.unique(y_original))  # Number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Label Encoder, encode the output\n",
    "iris_label_encoder = LabelEncoder()\n",
    "y = iris_label_encoder.fit_transform(y_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select training row indices: 0–40, 50–90, 100–140\n",
    "# Select test row indices: 40–50, 90–100, 140–150\n",
    "training_indices = np.r_[0:40, 50:90, 100:140]\n",
    "test_indices = np.setdiff1d(np.arange(len(y)), training_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 3 'binary' labels: one for each class.\n",
    "y_bin_class_0 = (y_original == 0).astype(int)\n",
    "y_bin_class_1 = (y_original == 1).astype(int)\n",
    "y_bin_class_2 = (y_original == 2).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get y-test/train split\n",
    "X_train, X_test, y_train, y_test = X[training_indices], X[test_indices], y[training_indices], y[test_indices]\n",
    "y_train_0, y_test_0 = y_bin_class_0[training_indices], y_bin_class_0[test_indices]\n",
    "y_train_1, y_test_1 = y_bin_class_1[training_indices], y_bin_class_1[test_indices]\n",
    "y_train_2, y_test_2 = y_bin_class_2[training_indices], y_bin_class_2[test_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying multi-class Classification\n",
    "This is example to use Multi-class classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_pipeline = Pipeline([\n",
    "    (\"scalar\" ,StandardScaler()),\n",
    "    (\"clf\", OneVsRestClassifier(LogisticRegression(solver=\"liblinear\", random_state=42)))\n",
    "]).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict probabilities\n",
    "y_pred_prob = sk_pipeline.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Equivalence\n",
    "Under the hood LogisticRegression creates multiple models. And uses various strategies to choose the winner. To compare apples-to-apples we will use same `solver=\"liblinear\"` and same pre-processing. The we shall compare the `predict_proba` for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 1, 0, 0, 0, 0]),\n",
       " array([0, 0, 1, 1, 0, 0]),\n",
       " array([0, 0, 0, 0, 1, 1]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print samples\n",
    "idx = [0, 1, 50, 51, 100, 101]\n",
    "(y_bin_class_0[idx], y_bin_class_1[idx], y_bin_class_2[idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "c0_pipeline = Pipeline([\n",
    "    (\"scalar\", StandardScaler()),\n",
    "    (\"clf\", LogisticRegression(solver=\"liblinear\", random_state=42))\n",
    "]).fit(X_train, y_train_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1_pipeline = Pipeline([\n",
    "    (\"scalar\", StandardScaler()),\n",
    "    (\"clf\", LogisticRegression(solver=\"liblinear\", random_state=42))\n",
    "]).fit(X_train, y_train_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "c2_pipeline = Pipeline([\n",
    "    (\"scalar\", StandardScaler()),\n",
    "    (\"clf\", LogisticRegression(solver=\"liblinear\", random_state=42))\n",
    "]).fit(X_train, y_train_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get probabilities, and combine them into one matrix to compare with sklearn's OVR\n",
    "y_pred_prob_class_0 = c0_pipeline.predict_proba(X_test)\n",
    "y_pred_prob_class_1 = c1_pipeline.predict_proba(X_test)\n",
    "y_pred_prob_class_2 = c2_pipeline.predict_proba(X_test)\n",
    "ovr_probs = np.column_stack([\n",
    "    y_pred_prob_class_0[:, 1],  # Probability of class 0\n",
    "    y_pred_prob_class_1[:, 1],  # Probability of class 1\n",
    "    y_pred_prob_class_2[:, 1]   # Probability of class 2\n",
    "])\n",
    "# Normalize the probabilities as sklearn does softmax in each row\n",
    "ovr_probs_normalized = ovr_probs / ovr_probs.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([30, 30, 30])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare the probabilities, and count the number of matches\n",
    "np.equal(ovr_probs_normalized, y_pred_prob).sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Remark:__ We see that all the \"30\" probabilities matched.\n",
    "\n",
    "## Printing Samples\n",
    "We shall look at prediction of first 2 samples of each class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.01108447, 0.98891553],\n",
       "        [0.22734722, 0.77265278]]),\n",
       " array([[0.89539545, 0.10460455],\n",
       "        [0.24965287, 0.75034713]]),\n",
       " array([[9.99638614e-01, 3.61386382e-04],\n",
       "        [9.99494295e-01, 5.05705487e-04]]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the binary classification first 2 rows of each class\n",
    "(y_pred_prob_class_0[:2], y_pred_prob_class_1[:2], y_pred_prob_class_2[:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[9.88915525e-01, 1.04604552e-01, 3.61386382e-04],\n",
       "        [7.72652776e-01, 7.50347131e-01, 5.05705487e-04]]),\n",
       " array([[0.0420859 , 0.59937954, 0.15443873],\n",
       "        [0.04810585, 0.34441794, 0.27194538]]),\n",
       " array([[0.00245132, 0.22650742, 0.95718486],\n",
       "        [0.00379213, 0.23693157, 0.90777971]]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the calculated first 2 rows of OVR\n",
    "(ovr_probs[:2], ovr_probs[10:12], ovr_probs[20:22])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[9.04042676e-01, 9.56269533e-02, 3.30370697e-04],\n",
       "        [5.07154532e-01, 4.92513532e-01, 3.31935428e-04]]),\n",
       " array([[0.0528781 , 0.75308003, 0.19404187],\n",
       "        [0.07239741, 0.51833547, 0.40926712]]),\n",
       " array([[0.00206663, 0.19096121, 0.80697216],\n",
       "        [0.0033018 , 0.20629592, 0.79040228]]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(ovr_probs_normalized[:2], ovr_probs_normalized[10:12], ovr_probs_normalized[20:22])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[9.04042676e-01, 9.56269533e-02, 3.30370697e-04],\n",
       "        [5.07154532e-01, 4.92513532e-01, 3.31935428e-04]]),\n",
       " array([[0.0528781 , 0.75308003, 0.19404187],\n",
       "        [0.07239741, 0.51833547, 0.40926712]]),\n",
       " array([[0.00206663, 0.19096121, 0.80697216],\n",
       "        [0.0033018 , 0.20629592, 0.79040228]]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print sklearn's OVR\n",
    "(y_pred_prob[:2], y_pred_prob[10:12], y_pred_prob[20:22])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
