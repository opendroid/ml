{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial Regresion\n",
    "\n",
    "When you apply `PolynomialFeatures` from scikit-learn to a dataset with say n=9 features $(X_1, X_2, \\ldots, X_9)$, it generates new features by creating all possible polynomial and interaction terms up to the specified `degree`.\n",
    "\n",
    "### How `PolynomialFeatures` Works\n",
    "- **Input**: An example dataset with 9 numerical features: $[X_1, X_2, X_3, X_4, X_5, X_6, X_7, X_8, X_9]$.\n",
    "- **Parameters**:\n",
    "  - `degree`: The maximum degree of the polynomial features (e.g., `degree=2` generates terms up to $(X_i^2)$.\n",
    "  - `interaction_only`: \n",
    "    - If `False` (default), includes all polynomial terms and interactions. \n",
    "    - If `True`, only includes interaction terms (e.g., $X_1 \\cdot X_2$, but not $X_1^2$).\n",
    "  - `include_bias`: If `True` (default), includes a constant term (1) as a feature.\n",
    "- **Output**: A new feature matrix containing:\n",
    "  - The original features.\n",
    "  - Polynomial terms (e.g., $X_1^2, X_1^3, \\ldots$) for each feature, up to `degree`.\n",
    "  - Interaction terms (e.g., $X_1 \\cdot X_2, X_1 \\cdot X_3, \\ldots$) for all feature combinations, up to `degree`.\n",
    "  - A bias term (if `include_bias=True`).\n",
    "\n",
    "### Number of Features Generated\n",
    "The number of output features depends on the `degree`, `interaction_only`, and `include_bias`. The formula for the total number of features (including the bias term) when `interaction_only=False` is given by the number of combinations of powers $a_1, a_2, \\ldots, a_9$ such that $0 \\leq a_1 + a_2 + \\cdots + a_9 \\leq \\text{degree}$, where $a_i$ are the powers of features $X_i$. Mathematically, for $n$ features (here, $n=9$) and degree $d$, the number of features (including bias) is: $\\binom{n + d}{d}$\n",
    "\n",
    "If `include_bias=False`, subtract 1 from the result. \n",
    "\n",
    "If `interaction_only=True`, the formula changes to focus only on interaction terms.\n",
    "\n",
    "### Recommendations\n",
    "- **Scaling**: Spply `StandardScaler` or `MinMaxScaler` **before** `PolynomialFeatures` to ensure features are on the same scale, preventing large polynomial terms from dominating.\n",
    "- **Feature Selection**: With 715 features for `degree=4`, consider dimensionality reduction (e.g., PCA) or feature selection (e.g., `SelectKBest`) after `PolynomialFeatures` to avoid overfitting and reduce computational cost.\n",
    "- **Sparsity**: If your data is sparse, set `PolynomialFeatures(sparse=True)` to maintain sparsity in the output (available in newer scikit-learn versions).\n",
    "- **Validation**: Test lower degrees (e.g., 2 or 3) first, as `degree=4` with 9 features may overfit unless you have a large dataset.\n",
    "- **include_bias=False**: as `LinearRegression(fit_intercept=True)` to avoid redundancy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X\n",
      "[[0.36842629 0.39172236 0.12117024]\n",
      " [0.30199636 0.92621505 0.99207497]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Simulated data: 2 samples, 1 bias, 2 features\n",
    "np.random.seed(282)\n",
    "X = np.random.rand(2, 3)\n",
    "print(\"X\")\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XPoly\n",
      "['X1' 'X2' 'X3']\n",
      "[[0.368 0.392 0.121]\n",
      " [0.302 0.926 0.992]]\n"
     ]
    }
   ],
   "source": [
    "# PolynomialFeatures with degree=1, include_bias=False, interaction_only=False\n",
    "# WIll result in exactly the same number of features as the original features\n",
    "poly1FF = PolynomialFeatures(degree=1, include_bias=False, interaction_only=False)\n",
    "X_poly1 = poly1FF.fit_transform(X)\n",
    "features = poly1FF.get_feature_names_out(input_features=['X1', 'X2', 'X3'])\n",
    "print(\"XPoly\")\n",
    "print(features)\n",
    "print(X_poly1.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(X, X_poly1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XPoly2\n",
      "['1' 'X1' 'X2' 'X3']\n",
      "[[1.    0.368 0.392 0.121]\n",
      " [1.    0.302 0.926 0.992]]\n"
     ]
    }
   ],
   "source": [
    "# Add degree=1, include_bias as True\n",
    "poly1TF = PolynomialFeatures(degree=1, include_bias=True, interaction_only=False)\n",
    "X_poly2 = poly1TF.fit_transform(X)\n",
    "features = poly1TF.get_feature_names_out(input_features=['X1', 'X2', 'X3'])\n",
    "print(\"XPoly2\")\n",
    "print(features)\n",
    "print(X_poly2.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XPoly1FT\n",
      "['X1' 'X2' 'X3']\n",
      "[[0.368 0.392 0.121]\n",
      " [0.302 0.926 0.992]]\n"
     ]
    }
   ],
   "source": [
    "# Add degree=1, interaction_only\n",
    "poly1FT = PolynomialFeatures(degree=1, include_bias=False, interaction_only=True)\n",
    "X_poly1FT = poly1FT.fit_transform(X)\n",
    "features = poly1FT.get_feature_names_out(input_features=['X1', 'X2', 'X3'])\n",
    "print(\"XPoly1FT\")\n",
    "print(features)\n",
    "print(X_poly1FT.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XPoly2FF\n",
      "['X1' 'X2' 'X3' 'X1^2' 'X1 X2' 'X1 X3' 'X2^2' 'X2 X3' 'X3^2' 'X1^3'\n",
      " 'X1^2 X2' 'X1^2 X3' 'X1 X2^2' 'X1 X2 X3' 'X1 X3^2' 'X2^3' 'X2^2 X3'\n",
      " 'X2 X3^2' 'X3^3']\n",
      "[[0.368 0.392 0.121 0.136 0.144 0.045 0.153 0.047 0.015 0.05  0.053 0.016\n",
      "  0.057 0.017 0.005 0.06  0.019 0.006 0.002]\n",
      " [0.302 0.926 0.992 0.091 0.28  0.3   0.858 0.919 0.984 0.028 0.084 0.09\n",
      "  0.259 0.277 0.297 0.795 0.851 0.912 0.976]]\n"
     ]
    }
   ],
   "source": [
    "# Add degree=3, include_bias as False, interaction_only False\n",
    "poly3FF = PolynomialFeatures(degree=3, include_bias=False, interaction_only=False)\n",
    "X_poly3FF = poly3FF.fit_transform(X)\n",
    "features = poly3FF.get_feature_names_out(input_features=['X1', 'X2', 'X3'])\n",
    "print(\"XPoly2FF\")\n",
    "print(features)\n",
    "print(X_poly3FF.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['X1' 'X2' 'X3' 'X4' 'X1^2' 'X1 X2' 'X1 X3' 'X1 X4' 'X2^2' 'X2 X3' 'X2 X4'\n",
      " 'X3^2' 'X3 X4' 'X4^2' 'X1^3' 'X1^2 X2' 'X1^2 X3' 'X1^2 X4' 'X1 X2^2'\n",
      " 'X1 X2 X3' 'X1 X2 X4' 'X1 X3^2' 'X1 X3 X4' 'X1 X4^2' 'X2^3' 'X2^2 X3'\n",
      " 'X2^2 X4' 'X2 X3^2' 'X2 X3 X4' 'X2 X4^2' 'X3^3' 'X3^2 X4' 'X3 X4^2'\n",
      " 'X4^3']\n"
     ]
    }
   ],
   "source": [
    "poly3FF = PolynomialFeatures(degree=3, include_bias=False, interaction_only=False)\n",
    "X1 = np.random.rand(1, 4)\n",
    "X_poly3FF = poly3FF.fit_transform(X1)\n",
    "features = poly3FF.get_feature_names_out(input_features=['X1', 'X2', 'X3', 'X4'])\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to apply Polynomial Transformation\n",
    "Polynomial regression (using PolynomialFeatures with a linear model like LinearRegression or Ridge) is appropriate when:\n",
    "\n",
    "1. **Non-linear Relationships:** The relationship between features and the target (house value) is non-linear (e.g., house value may increase quadratically with income up to a point).\n",
    "2. **Feature Interactions:** Interactions between features (e.g., Latitude * Longitude) capture combined effects critical to the target.\n",
    "3. **Sufficient Data:** The dataset (~20,640 samples) can support additional features without severe overfitting, especially with regularization.\n",
    "4. **Domain Knowledge:** Features like income or location are known to have non-linear or interactive effects on house prices.\n",
    "\n",
    "### When to Avoid Polynomial Regression:\n",
    "\n",
    "1. **Noisy or Weak Features:** Polynomial terms for noisy or weakly predictive features amplify noise (e.g., Populationâ€™s outliers).\n",
    "2. **High Dimensionality:** Applying high-degree polynomials to all features causes feature explosion (e.g., degree=2 for 8 features yields 45 features), risking overfitting and computational cost.\n",
    "3. **Categorical Features:** Polynomial terms are meaningless for one-hot encoded variables like ocean_proximity."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
